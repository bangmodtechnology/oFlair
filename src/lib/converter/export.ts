/**
 * Export utilities for downloading generated DAGs
 */

import type { GeneratedDAG } from "@/types/airflow";
import { formatReportAsText, formatReportAsJson, type ConversionReport } from "./report";

/**
 * Download a single file
 */
export function downloadFile(content: string, filename: string, mimeType: string = "text/plain"): void {
  const blob = new Blob([content], { type: mimeType });
  const url = URL.createObjectURL(blob);
  const link = document.createElement("a");
  link.href = url;
  link.download = filename;
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  URL.revokeObjectURL(url);
}

/**
 * Download a single DAG as Python file
 */
export function downloadDag(dag: GeneratedDAG): void {
  downloadFile(dag.content, dag.filename, "text/x-python");
}

/**
 * Download all DAGs as a ZIP file
 */
export async function downloadAllAsZip(
  dags: GeneratedDAG[],
  report?: ConversionReport,
  options?: {
    includeReport?: boolean;
    folderName?: string;
  }
): Promise<void> {
  const { includeReport = true, folderName = "airflow_dags" } = options || {};

  // Dynamically import JSZip
  const JSZip = (await import("jszip")).default;
  const zip = new JSZip();

  // Create a folder for DAGs
  const dagsFolder = zip.folder(folderName);

  if (!dagsFolder) {
    throw new Error("Failed to create ZIP folder");
  }

  // Add each DAG file
  for (const dag of dags) {
    dagsFolder.file(dag.filename, dag.content);
  }

  // Add report if requested
  if (includeReport && report) {
    dagsFolder.file("CONVERSION_REPORT.txt", formatReportAsText(report));
    dagsFolder.file("conversion_report.json", formatReportAsJson(report));
  }

  // Add README
  const readme = generateReadme(dags, report);
  dagsFolder.file("README.md", readme);

  // Add requirements.txt
  const requirements = generateRequirements(dags);
  dagsFolder.file("requirements.txt", requirements);

  // Generate ZIP and download
  const blob = await zip.generateAsync({ type: "blob" });
  const url = URL.createObjectURL(blob);
  const link = document.createElement("a");
  link.href = url;
  link.download = `${folderName}.zip`;
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  URL.revokeObjectURL(url);
}

/**
 * Generate README.md content
 */
function generateReadme(dags: GeneratedDAG[], report?: ConversionReport): string {
  const lines: string[] = [];

  lines.push("# Generated Airflow DAGs");
  lines.push("");
  lines.push("This folder contains Airflow DAGs generated from Control-M job definitions using OFlair.");
  lines.push("");
  lines.push("## DAGs Included");
  lines.push("");

  for (const dag of dags) {
    lines.push(`- **${dag.filename}**: ${dag.dag.description || dag.dag.dagId}`);
    lines.push(`  - Tasks: ${dag.dag.tasks.length}`);
    lines.push(`  - Schedule: ${dag.dag.schedule || "None"}`);
  }

  lines.push("");
  lines.push("## Installation");
  lines.push("");
  lines.push("1. Copy the `.py` files to your Airflow DAGs folder");
  lines.push("2. Install required dependencies: `pip install -r requirements.txt`");
  lines.push("3. Restart Airflow scheduler to pick up the new DAGs");
  lines.push("");
  lines.push("## Configuration");
  lines.push("");
  lines.push("You may need to configure the following:");
  lines.push("");
  lines.push("- **Connections**: Set up Airflow connections for operators that require them");
  lines.push("- **Variables**: Configure any required Airflow variables");
  lines.push("- **Pools**: Set up pools if your tasks require resource management");
  lines.push("");

  if (report) {
    lines.push("## Conversion Summary");
    lines.push("");
    lines.push(`- Total Jobs: ${report.summary.totalJobs}`);
    lines.push(`- Converted: ${report.summary.convertedJobs}`);
    lines.push(`- Conversion Rate: ${report.summary.conversionRate}%`);
    lines.push(`- Warnings: ${report.warnings.length}`);
    lines.push(`- Manual Review: ${report.manualReviewRequired.length}`);
    lines.push("");
    lines.push("See `CONVERSION_REPORT.txt` for detailed conversion information.");
    lines.push("");
  }

  lines.push("## Generated By");
  lines.push("");
  lines.push("[OFlair](https://github.com/bangmodtech/oflair) - Control-M to Airflow Converter");
  lines.push("");
  lines.push(`Generated: ${new Date().toISOString()}`);

  return lines.join("\n");
}

/**
 * Generate requirements.txt based on operators used
 */
function generateRequirements(dags: GeneratedDAG[]): string {
  const requirements = new Set<string>();

  // Base requirement
  requirements.add("apache-airflow>=2.5.0");

  // Check operators used across all DAGs
  const operatorTypes = new Set<string>();
  for (const dag of dags) {
    for (const task of dag.dag.tasks) {
      operatorTypes.add(task.operatorType);
    }
  }

  // Add provider packages based on operators
  if (operatorTypes.has("KubernetesPodOperator")) {
    requirements.add("apache-airflow-providers-cncf-kubernetes");
  }
  if (operatorTypes.has("WasbBlobSensor") || operatorTypes.has("WasbBlobUploadOperator")) {
    requirements.add("apache-airflow-providers-microsoft-azure");
  }
  if (operatorTypes.has("SSHOperator")) {
    requirements.add("apache-airflow-providers-ssh");
  }
  if (operatorTypes.has("SimpleHttpOperator")) {
    requirements.add("apache-airflow-providers-http");
  }
  if (operatorTypes.has("SQLExecuteQueryOperator")) {
    requirements.add("apache-airflow-providers-common-sql");
  }
  if (operatorTypes.has("S3KeySensor")) {
    requirements.add("apache-airflow-providers-amazon");
  }
  if (operatorTypes.has("GCSObjectExistenceSensor")) {
    requirements.add("apache-airflow-providers-google");
  }

  return Array.from(requirements).join("\n");
}

/**
 * Export report as downloadable file
 */
export function downloadReport(report: ConversionReport, format: "txt" | "json" = "txt"): void {
  if (format === "json") {
    downloadFile(formatReportAsJson(report), "conversion_report.json", "application/json");
  } else {
    downloadFile(formatReportAsText(report), "CONVERSION_REPORT.txt", "text/plain");
  }
}

/**
 * Copy DAG code to clipboard
 */
export async function copyToClipboard(content: string): Promise<boolean> {
  try {
    await navigator.clipboard.writeText(content);
    return true;
  } catch (error) {
    console.error("Failed to copy to clipboard:", error);
    return false;
  }
}

import Handlebars from "handlebars";
import type { ControlMJob } from "@/types/controlm";
import type { AirflowDAG, AirflowTask, AirflowDependency } from "@/types/airflow";

// Register Handlebars helpers
Handlebars.registerHelper("camel_case", (str: string) => {
  if (!str) return "";
  return str
    .replace(/[-_\s]+(.)?/g, (_, c) => (c ? c.toUpperCase() : ""))
    .replace(/^(.)/, (c) => c.toLowerCase());
});

Handlebars.registerHelper("quote", (str: string) => {
  if (!str) return '""';
  return `"${str.replace(/"/g, '\\"')}"`;
});

Handlebars.registerHelper("escape_python", (str: string) => {
  if (!str) return "";
  return str.replace(/\\/g, "\\\\").replace(/'/g, "\\'").replace(/"/g, '\\"');
});

// Airflow 2.x DAG Template
const DAG_TEMPLATE_V2 = `"""
Auto-generated Airflow DAG from Control-M
Generated by OFlair
DAG: {{dag_id}}
Airflow Version: 2.x
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.email import EmailOperator
from airflow.sensors.filesystem import FileSensor
{{#if has_kubernetes}}
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
{{/if}}
{{#if has_azure}}
from airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor
{{/if}}
{{#if has_ssh}}
from airflow.providers.ssh.operators.ssh import SSHOperator
{{/if}}

# Default arguments
default_args = {
    'owner': '{{owner}}',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': {{retries}},
    'retry_delay': timedelta(minutes={{retry_delay}}),
}

# DAG definition
with DAG(
    dag_id='{{dag_id}}',
    default_args=default_args,
    description='{{description}}',
    schedule={{schedule}},
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags={{tags}},
) as dag:

{{#each tasks}}
    # Task: {{task_id}}
{{#if is_bash}}
    {{task_id}} = BashOperator(
        task_id='{{task_id}}',
        bash_command='''{{bash_command}}''',
{{#if env_vars}}
        env={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
{{#if pool}}
        pool='{{pool}}',
{{/if}}
    )
{{/if}}
{{#if is_python}}
    {{task_id}} = PythonOperator(
        task_id='{{task_id}}',
        python_callable={{python_callable}},
{{#if pool}}
        pool='{{pool}}',
{{/if}}
    )
{{/if}}
{{#if is_sensor}}
    {{task_id}} = FileSensor(
        task_id='{{task_id}}',
        filepath='{{filepath}}',
        poke_interval=60,
        timeout=3600,
        mode='poke',
    )
{{/if}}
{{#if is_empty}}
    {{task_id}} = EmptyOperator(
        task_id='{{task_id}}',
    )
{{/if}}
{{#if is_kubernetes}}
    {{task_id}} = KubernetesPodOperator(
        task_id='{{task_id}}',
        name='{{task_id}}-pod',
        namespace='{{namespace}}',
        image='{{image}}',
{{#if cmds}}
        cmds=['/bin/bash', '-c'],
        arguments=['''{{cmds}}'''],
{{/if}}
{{#if env_vars}}
        env_vars={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
        get_logs=True,
        is_delete_operator_pod=True,
    )
{{/if}}
{{#if is_azure}}
    {{task_id}} = WasbBlobSensor(
        task_id='{{task_id}}',
        container_name='{{container_name}}',
        blob_name='{{blob_name}}',
        wasb_conn_id='azure_blob_default',
    )
{{/if}}
{{#if is_ssh}}
    {{task_id}} = SSHOperator(
        task_id='{{task_id}}',
        ssh_conn_id='ssh_default',
        command='''{{ssh_command}}''',
{{#if remote_host}}
        remote_host='{{remote_host}}',
{{/if}}
    )
{{/if}}
{{#if is_email}}
    {{task_id}} = EmailOperator(
        task_id='{{task_id}}',
        to=['admin@example.com'],
        subject='{{subject}}',
        html_content='<p>Job completed</p>',
    )
{{/if}}

{{/each}}
    # Dependencies
{{#each dependencies}}
    {{upstream}} >> {{downstream}}
{{/each}}
{{#unless dependencies}}
    # No dependencies defined
    pass
{{/unless}}
`;

// Airflow 3.x DAG Template
const DAG_TEMPLATE_V3 = `"""
Auto-generated Airflow DAG from Control-M
Generated by OFlair
DAG: {{dag_id}}
Airflow Version: 3.x
"""

from datetime import datetime, timedelta
from airflow.sdk import DAG, Asset
from airflow.providers.standard.operators.bash import BashOperator
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.providers.standard.operators.email import EmailOperator
from airflow.providers.standard.sensors.filesystem import FileSensor
{{#if has_kubernetes}}
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
{{/if}}
{{#if has_azure}}
from airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor
{{/if}}
{{#if has_ssh}}
from airflow.providers.ssh.operators.ssh import SSHOperator
{{/if}}

# Default arguments
default_args = {
    'owner': '{{owner}}',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': {{retries}},
    'retry_delay': timedelta(minutes={{retry_delay}}),
}

# DAG definition
with DAG(
    dag_id='{{dag_id}}',
    default_args=default_args,
    description='{{description}}',
    schedule={{schedule}},
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags={{tags}},
) as dag:

{{#each tasks}}
    # Task: {{task_id}}
{{#if is_bash}}
    {{task_id}} = BashOperator(
        task_id='{{task_id}}',
        bash_command='''{{bash_command}}''',
{{#if env_vars}}
        env={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
{{#if pool}}
        pool='{{pool}}',
{{/if}}
    )
{{/if}}
{{#if is_python}}
    {{task_id}} = PythonOperator(
        task_id='{{task_id}}',
        python_callable={{python_callable}},
{{#if pool}}
        pool='{{pool}}',
{{/if}}
    )
{{/if}}
{{#if is_sensor}}
    {{task_id}} = FileSensor(
        task_id='{{task_id}}',
        filepath='{{filepath}}',
        poke_interval=60,
        timeout=3600,
        mode='poke',
    )
{{/if}}
{{#if is_empty}}
    {{task_id}} = EmptyOperator(
        task_id='{{task_id}}',
    )
{{/if}}
{{#if is_kubernetes}}
    {{task_id}} = KubernetesPodOperator(
        task_id='{{task_id}}',
        name='{{task_id}}-pod',
        namespace='{{namespace}}',
        image='{{image}}',
{{#if cmds}}
        cmds=['/bin/bash', '-c'],
        arguments=['''{{cmds}}'''],
{{/if}}
{{#if env_vars}}
        env_vars={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
        get_logs=True,
        is_delete_operator_pod=True,
    )
{{/if}}
{{#if is_azure}}
    {{task_id}} = WasbBlobSensor(
        task_id='{{task_id}}',
        container_name='{{container_name}}',
        blob_name='{{blob_name}}',
        wasb_conn_id='azure_blob_default',
    )
{{/if}}
{{#if is_ssh}}
    {{task_id}} = SSHOperator(
        task_id='{{task_id}}',
        ssh_conn_id='ssh_default',
        command='''{{ssh_command}}''',
{{#if remote_host}}
        remote_host='{{remote_host}}',
{{/if}}
    )
{{/if}}
{{#if is_email}}
    {{task_id}} = EmailOperator(
        task_id='{{task_id}}',
        to=['admin@example.com'],
        subject='{{subject}}',
        html_content='<p>Job completed</p>',
    )
{{/if}}

{{/each}}
    # Dependencies
{{#each dependencies}}
    {{upstream}} >> {{downstream}}
{{/each}}
{{#unless dependencies}}
    # No dependencies defined
    pass
{{/unless}}
`;

// Airflow 3.x TaskFlow API Template (decorator style)
const DAG_TEMPLATE_V3_TASKFLOW = `"""
Auto-generated Airflow DAG from Control-M
Generated by OFlair (TaskFlow API)
DAG: {{dag_id}}
Airflow Version: 3.x
"""

from datetime import datetime, timedelta
from airflow.sdk import dag, task
from airflow.providers.standard.operators.bash import BashOperator
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.providers.standard.operators.email import EmailOperator
from airflow.providers.standard.sensors.filesystem import FileSensor
{{#if has_kubernetes}}
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
{{/if}}
{{#if has_azure}}
from airflow.providers.microsoft.azure.sensors.wasb import WasbBlobSensor
{{/if}}
{{#if has_ssh}}
from airflow.providers.ssh.operators.ssh import SSHOperator
{{/if}}

default_args = {
    'owner': '{{owner}}',
    'depends_on_past': False,
    'retries': {{retries}},
    'retry_delay': timedelta(minutes={{retry_delay}}),
}

@dag(
    dag_id='{{dag_id}}',
    default_args=default_args,
    description='{{description}}',
    schedule={{schedule}},
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags={{tags}},
)
def {{dag_id}}_workflow():
    """{{description}}"""

{{#each tasks}}
{{#if is_bash}}
    {{task_id}} = BashOperator(
        task_id='{{task_id}}',
        bash_command='''{{bash_command}}''',
{{#if env_vars}}
        env={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
    )
{{/if}}
{{#if is_python}}
    @task
    def {{task_id}}_func():
        """Task: {{task_id}}"""
        # TODO: Implement python logic
        pass

    {{task_id}} = {{task_id}}_func()
{{/if}}
{{#if is_sensor}}
    {{task_id}} = FileSensor(
        task_id='{{task_id}}',
        filepath='{{filepath}}',
        poke_interval=60,
        timeout=3600,
    )
{{/if}}
{{#if is_empty}}
    {{task_id}} = EmptyOperator(
        task_id='{{task_id}}',
    )
{{/if}}
{{#if is_kubernetes}}
    {{task_id}} = KubernetesPodOperator(
        task_id='{{task_id}}',
        name='{{task_id}}-pod',
        namespace='{{namespace}}',
        image='{{image}}',
{{#if cmds}}
        cmds=['/bin/bash', '-c'],
        arguments=['''{{cmds}}'''],
{{/if}}
{{#if env_vars}}
        env_vars={
{{#each env_vars}}
            '{{name}}': '{{value}}',
{{/each}}
        },
{{/if}}
        get_logs=True,
        is_delete_operator_pod=True,
    )
{{/if}}
{{#if is_azure}}
    {{task_id}} = WasbBlobSensor(
        task_id='{{task_id}}',
        container_name='{{container_name}}',
        blob_name='{{blob_name}}',
        wasb_conn_id='azure_blob_default',
    )
{{/if}}
{{#if is_ssh}}
    {{task_id}} = SSHOperator(
        task_id='{{task_id}}',
        ssh_conn_id='ssh_default',
        command='''{{ssh_command}}''',
{{#if remote_host}}
        remote_host='{{remote_host}}',
{{/if}}
    )
{{/if}}
{{#if is_email}}
    {{task_id}} = EmailOperator(
        task_id='{{task_id}}',
        to=['admin@example.com'],
        subject='{{subject}}',
        html_content='<p>Job completed</p>',
    )
{{/if}}

{{/each}}
    # Set dependencies
{{#each dependencies}}
    {{upstream}} >> {{downstream}}
{{/each}}

# Instantiate the DAG
{{dag_id}}_dag = {{dag_id}}_workflow()
`;

export type AirflowVersion = "2.5" | "2.6" | "2.7" | "2.8" | "2.9" | "2.10" | "3.0" | "3.1";

export interface GeneratorOptions {
  airflowVersion?: AirflowVersion;
  useTaskFlowApi?: boolean;
}

export function generateDagCode(dag: AirflowDAG, options: GeneratorOptions = {}): string {
  const { airflowVersion = "2.9", useTaskFlowApi = false } = options;

  // Select template based on version
  let templateStr: string;
  const majorVersion = parseInt(airflowVersion.split(".")[0]);

  if (majorVersion >= 3) {
    templateStr = useTaskFlowApi ? DAG_TEMPLATE_V3_TASKFLOW : DAG_TEMPLATE_V3;
  } else {
    templateStr = DAG_TEMPLATE_V2;
  }

  const template = Handlebars.compile(templateStr);

  // Check for special operators
  const hasKubernetes = dag.tasks.some((t) => t.operatorType === "KubernetesPodOperator");
  const hasAzure = dag.tasks.some((t) => t.operatorType === "WasbBlobSensor");
  const hasSsh = dag.tasks.some((t) => t.operatorType === "SSHOperator");

  const tasks = dag.tasks.map((task) => ({
    ...task,
    is_bash: task.operatorType === "BashOperator",
    is_python: task.operatorType === "PythonOperator",
    is_sensor: task.operatorType === "FileSensor",
    is_empty: task.operatorType === "EmptyOperator",
    is_kubernetes: task.operatorType === "KubernetesPodOperator",
    is_azure: task.operatorType === "WasbBlobSensor",
    is_ssh: task.operatorType === "SSHOperator",
    is_email: task.operatorType === "EmailOperator",
    bash_command: task.params.bash_command || task.params.command || "echo 'No command specified'",
    python_callable: task.params.python_callable || "lambda: None",
    filepath: task.params.filepath || "/tmp/watched_file",
    namespace: task.params.namespace || "default",
    image: task.params.image || "python:3.9",
    cmds: task.params.cmds || task.params.bash_command,
    container_name: task.params.container_name || "default-container",
    blob_name: task.params.blob_name || task.params.filepath,
    ssh_command: task.params.ssh_command || task.params.bash_command,
    remote_host: task.params.remote_host,
    subject: task.params.subject || "Airflow Notification",
    env_vars: task.params.env_vars as Array<{ name: string; value: string }> | undefined,
    pool: task.pool,
  }));

  return template({
    dag_id: dag.dagId,
    description: dag.description || `Converted from Control-M`,
    owner: dag.defaultArgs?.owner || "airflow",
    retries: dag.defaultArgs?.retries ?? 1,
    retry_delay: dag.defaultArgs?.retryDelay ?? 5,
    schedule: dag.schedule || "None",
    tags: JSON.stringify(dag.tags || ["control-m-migration"]),
    tasks,
    dependencies: dag.dependencies,
    has_kubernetes: hasKubernetes,
    has_azure: hasAzure,
    has_ssh: hasSsh,
  });
}

export function convertJobToTask(job: ControlMJob): AirflowTask {
  const taskId = normalizeTaskId(job.JOBNAME);
  const jobType = (job.JOB_TYPE || "Command").toLowerCase();

  // Determine operator type based on job type
  let operatorType: AirflowTask["operatorType"] = "BashOperator";
  const params: Record<string, unknown> = {};

  // Convert VARIABLE tags to env_vars
  if (job.VARIABLE && job.VARIABLE.length > 0) {
    params.env_vars = job.VARIABLE.map((v) => ({
      name: v.NAME,
      value: v.VALUE,
    }));
  }

  if (jobType.includes("kubernetes") || jobType.includes("container") || jobType.includes("docker")) {
    operatorType = "KubernetesPodOperator";
    params.cmds = job.CMDLINE;
    params.image = job.FILENAME || "python:3.9";
    params.namespace = job.HOST || "default";
  } else if (jobType.includes("azure") || jobType.includes("blob")) {
    operatorType = "WasbBlobSensor";
    params.blob_name = job.FILENAME;
    params.container_name = job.HOST || "default-container";
  } else if (jobType.includes("ssh") || jobType.includes("remote")) {
    operatorType = "SSHOperator";
    params.ssh_command = job.CMDLINE;
    params.remote_host = job.HOST;
  } else if (jobType.includes("email") || jobType.includes("mail") || jobType.includes("notification")) {
    operatorType = "EmailOperator";
    params.subject = job.DESCRIPTION || "Airflow Notification";
  } else if (jobType.includes("file") || jobType.includes("watcher")) {
    operatorType = "FileSensor";
    params.filepath = job.FILENAME || "/tmp/watched_file";
  } else if (jobType.includes("python") || jobType.includes("script")) {
    if (job.FILENAME?.endsWith(".py")) {
      operatorType = "PythonOperator";
      params.python_callable = `run_${taskId}`;
    } else {
      operatorType = "BashOperator";
      params.bash_command = job.CMDLINE || job.FILENAME || "echo 'No command'";
    }
  } else if (jobType === "dummy" || jobType === "box") {
    operatorType = "EmptyOperator";
  } else {
    // Default to BashOperator
    operatorType = "BashOperator";
    params.bash_command = job.CMDLINE || job.FILENAME || "echo 'No command'";
  }

  return {
    taskId,
    operatorType,
    description: job.DESCRIPTION,
    params,
    priority: job.PRIORITY ? parseInt(job.PRIORITY) : undefined,
  };
}

export function buildDependencies(jobs: ControlMJob[]): AirflowDependency[] {
  const dependencies: AirflowDependency[] = [];
  const jobNameToTaskId = new Map<string, string>();

  // Build job name to task ID mapping
  for (const job of jobs) {
    jobNameToTaskId.set(job.JOBNAME, normalizeTaskId(job.JOBNAME));
  }

  // Build dependencies from INCOND (input conditions)
  for (const job of jobs) {
    const taskId = normalizeTaskId(job.JOBNAME);

    if (job.INCOND) {
      for (const cond of job.INCOND) {
        // Find the job that produces this condition
        const producerJob = jobs.find(
          (j) => j.OUTCOND?.some((oc) => oc.NAME === cond.NAME)
        );

        if (producerJob) {
          const upstreamTaskId = normalizeTaskId(producerJob.JOBNAME);
          // Avoid self-references
          if (upstreamTaskId !== taskId) {
            dependencies.push({
              upstream: upstreamTaskId,
              downstream: taskId,
            });
          }
        }
      }
    }
  }

  return dependencies;
}

function normalizeTaskId(str: string): string {
  return str
    .replace(/[-\s]+/g, "_")
    .replace(/[^a-zA-Z0-9_]/g, "")
    .toLowerCase();
}

export function groupJobsByFolder(
  jobs: ControlMJob[]
): Map<string, ControlMJob[]> {
  const groups = new Map<string, ControlMJob[]>();

  for (const job of jobs) {
    const folder = job.FOLDER_NAME || "default";
    const existing = groups.get(folder) || [];
    existing.push(job);
    groups.set(folder, existing);
  }

  return groups;
}
